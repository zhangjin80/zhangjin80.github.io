<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[归并排序算法实现详细注释]]></title>
    <url>%2F2018%2F03%2F09%2F%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E8%AF%A6%E7%BB%86%E6%B3%A8%E9%87%8A%2F</url>
    <content type="text"><![CDATA[归并排序的思想：分而治之。把一个庞大的数列从中间切分成两个，再分别对这两个数列排序，排序完后再合并，效率会比直接排序高。 那进一步把左右两边不断切分下去，效率也就会越高，切到只剩下一个元素是就不用再进行排序了。这是递归的思想，这就是所谓的归。 合并思想：把一个无序数列，从中间分开，分成左右两边，并递归的对左边和右边进行切分，直到到只剩下一个元素，这时一个元素的序列是有序的，不要再排序了，只需要向上和其盘边的序列进行合并就行了这就是并。 这里整理了一份归并排序的代码并写了详细的注释。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990public class MergeSort&#123; public static void main(String[] args) &#123;// 初始化一个数组 int[] intArray = &#123;4,2,5,3,3,10,13,4,15,6,13,20,11&#125;; // 先打印未排序的数组 System.out.println(Arrays.toString(intArray)); // 归并排序 mergeSort(intArray); // 打印排序好了的数组 System.out.println(Arrays.toString(intArray)); &#125; public static void mergeSort(int a[])&#123; /** * 为什么要给定下标范围： * 要把一个序列分而治之，需要分别对他的左边和右边切分 */ mergesort(a,0,a.length-1); &#125; /** * 递归方法： * 递归的对数组进行对半切分 * @param a 要排序的数组 * @param first 要排序的数组起始位置 * @param last 要排序的数组的终止位置 * 以两个元素为例进行递归演示： * mergesort&#123;1,2&#125;,0,1 * mid = 0 * mergesort(&#123;1,2&#125;,0,0)---first=last出递归 * mergesort(&#123;1,2&#125;,1,1)---first=last出递归 * mergearray(&#123;1,2&#125;,0,0,1)---放入切分的中间标记，数组左右两边进行合并 */ private static void mergesort(int[] a, int first, int last) &#123; if(first&lt;last)&#123;//递归出口，first=last，只有一个元素时返回继续向下执行 /** * 为什么不能用a.leng/2呢？ * 在递归调用时传的是都是原始数组a，如果用leng的话mid值不会变， * 起不到切分的作用 */ int mid = (first+last)/2; mergesort(a,first,mid); mergesort(a,mid+1,last); mergearray(a,first,mid,last); &#125; &#125; /** * 合并方法： * 把一个数组切分后有序的两端进行合并 * @param a 原始数组 * @param first 要合并的左边数组的开始位置 * @param mid 切分点 * @param last 要合并的右边数组末尾位置 */ private static void mergearray(int[] a, int first, int mid, int last) &#123; /** * 临时数组用于存放有序元素 * 为什么不用int[a.length]创建？ * 因为每次合并的只是a的一部分而已，这样做可以节省空间 */ int[] temp = new int[last+1]; int i = first;//左边数组起始位置 int j = mid+1;//右边数组起始位置 int m = mid;//左边数组终止位置 int n = last;//右边数组终止位置 int k = 0;//临时数组的指针 /** * 比较两个数组栈顶元素大小，把小的存入临时数组 */ while(i&lt;=m&amp;&amp;j&lt;=n)&#123; if(a[i]&lt;=a[j]) temp[k++] = a[i++]; else temp[k++] = a[j++]; &#125; /** * 分别把两个数组剩下的元素存入临时数组 */ while(i&lt;=m)&#123; temp[k++]=a[i++]; &#125; while(j&lt;=n)&#123; temp[k++] = a[j++]; &#125; /** * 把临时数组的值赋给原来的数组 */ for(i=0;i&lt;k;i++)&#123; a[first+i] = temp[i]; &#125; &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二次排序算法（可求不同类别下的Top N）]]></title>
    <url>%2F2018%2F01%2F28%2F%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%EF%BC%88%E5%8F%AF%E6%B1%82%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%88%AB%E4%B8%8B%E7%9A%84Top-N%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1234一条测试数据示例：math,xuzheng,54,52,86,91,42,85,75课程名，学生姓名，分数（完整的数据放在最后） 需求：求出每门课程参考学生平均成绩最高的学生的信息：课程，姓名和平均分。 思路： 创建课程pojo类，实现WritableComparable接口，实现compareTo方法，先对课程名进行比较，相同再对分数进行比较。 创建分组类实现WritableComparator接口，实现compare方法对课程名进行比较。 在map()中对每行数据进切片，求出平均分，封装进课程对象，然后把改对象作为outKey输出给reduce()。 reduce()每一组的values进行遍历，输出这一组排序好的key。 具体实现课程类CourseScore123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class CourseScore implements WritableComparable&lt;CourseScore&gt;&#123; private String courseName; private String studentName; private double avgScore; public CourseScore() &#123; &#125; //根据最后的输出格式实现toString方法 @Override public String toString() &#123; return courseName + "\t" + studentName + "\t" + avgScore; &#125; //对属性序列化 @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(courseName); out.writeUTF(studentName); out.writeDouble(avgScore); &#125; //反序列化 @Override public void readFields(DataInput in) throws IOException &#123; this.courseName = in.readUTF(); this.studentName = in.readUTF(); this.avgScore = in.readDouble(); &#125; //实现比较器 @Override public int compareTo(CourseScore o) &#123; //先比较课程名 int result = o.courseName.compareTo(this.courseName); //课程名相同，进行平均分的比较 if(result==0)&#123; double temp = o.avgScore-this.avgScore; if(temp==0)&#123; return 0; &#125;else&#123; return temp&gt;0?1:-1; &#125; &#125;else&#123; return result; &#125; &#125; public String getCourseName() &#123; return courseName; &#125; public void setCourseName(String courseName) &#123; this.courseName = courseName; &#125; public String getStudentName() &#123; return studentName; &#125; public void setStudentName(String studentName) &#123; this.studentName = studentName; &#125; public double getAvgScore() &#123; return avgScore; &#125; public void setAvgScore(double avgScore) &#123; this.avgScore = avgScore; &#125;&#125; 分组类1234567891011121314public class ClazzScoreGroupComparator extends WritableComparator &#123; //默认构造器需要调用父类的构造方法 public ClazzScoreGroupComparator() &#123; super(CourseScore.class,true); &#125; //实现分组比较器根据课程名分组就比较课程名 @Override public int compare(WritableComparable a, WritableComparable b) &#123; CourseScore cs1 = (CourseScore)a; CourseScore cs2 = (CourseScore)b; return cs1.getCourseName().compareTo(cs2.getCourseName()); &#125;&#125; MapReduce12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class SecondSort &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); Job job = Job.getInstance(conf); job.setJarByClass(SecondSort.class); job.setMapperClass(MR_Mapper.class); job.setReducerClass(MR_Reducer.class); job.setOutputKeyClass(CourseScore.class); job.setOutputValueClass(NullWritable.class); //设置分组比较器 job.setGroupingComparatorClass(ClazzScoreGroupComparator.class); Path inputPath = new Path("D:\\bigdata\\flow\\input\\grad"); Path outputPath = new Path("D:\\bigdata\\flow\\output\\q3"); FileInputFormat.setInputPaths(job, inputPath); if(fs.exists(outputPath))&#123; fs.delete(outputPath, true); &#125; FileOutputFormat.setOutputPath(job, outputPath); boolean isDone = job.waitForCompletion(true); System.exit(isDone ? 0 : 1); &#125; /** * 原始数据示例： * math,liujialing,85,86,41,75,93,42,85,75 * english,huangxiaoming,85,86,41,75,93,42,85 */ public static class MR_Mapper extends Mapper&lt;LongWritable, Text, CourseScore, NullWritable&gt;&#123; //创建课程分数类对象 CourseScore cs = new CourseScore(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] split = value.toString().split(","); String courseName = split[0]; String studentName = split[1]; double avgScore=0; int sum = 0; for(int i=2;i&lt;split.length;i++)&#123; sum += Integer.parseInt(split[i]); &#125; //求平均分 avgScore=sum/(split.length-2.0); avgScore =(Math.round(avgScore*100)/100.0);//保留两位小数 //把平均分，课程名，学生姓名封装进对象 cs.setAvgScore(avgScore); cs.setCourseName(courseName); cs.setStudentName(studentName); context.write(cs, NullWritable.get()); &#125; &#125; public static class MR_Reducer extends Reducer&lt;CourseScore, NullWritable, CourseScore, NullWritable&gt;&#123; @Override protected void reduce(CourseScore key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; //如果只想输出前N项可以设置计数器int count = 0，放在for循环中为N的时候退出 //遍历每一组的values并输出这一组key for(NullWritable v:values)&#123; context.write(key, NullWritable.get()); &#125; &#125; &#125;&#125; 数据流分析 原始数据： 123456computer,huangxiaoming,85,86,41,75,93,42,85english,yangmi,85,41,75,21,85,96,14english,huangdatou,48,58,67,86,15,33,85computer,xuzheng,54,52,86,91,42computer,huangbo,85,42,96,38english,liuyifei,76,95,86,74,68,74,48 map切分后封装到对象中的数据： 123456computer huangxiaoming 72.43english liuyifei 74.43english huangdatou 56.0computer xuzheng 65.0computer huangbo 65.25english yangmi 59.57 分组后的数据 1234567computer huangxiaoming 72.43computer huangbo 65.25computer xuzheng 65.0english liuyifei 74.43english yangmi 59.57english huangdatou 56.0 一般而言，reduce阶段会把具有相同map阶段输出的具有相同key的数据进行聚合，本例中map输出的每一个key都是不同的，但是设置了分组后，reduce就会把每一组的数据进行聚合，我们只要在迭代这组数据的时候输出key，就能把每一组中已经排序好的，不同的key进行输出。 如果不迭代values，直接输出key的话，只会输出这一组的数据的最后一个key，因为这一组前面的数据会被后面的数据覆盖。如在上述示例数据中只会输出：computer,xuzheng,65.0english,huangdatou,56.0 注意： 假如排序规则需要:a b c d 那么分组规则只能从前往后比较 也就是只有如下选择： 123456 - a - a b //先比较a再比较b - a b c //先比较a再比较b再比较c - a b c d ``` ## 测试数据 computer,huangxiaoming,85,86,41,75,93,42,85computer,xuzheng,54,52,86,91,42computer,huangbo,85,42,96,38english,zhaobenshan,54,52,86,91,42,85,75english,liuyifei,85,41,75,21,85,96,14algorithm,liuyifei,75,85,62,48,54,96,15computer,huangjiaju,85,75,86,85,85english,liuyifei,76,95,86,74,68,74,48english,huangdatou,48,58,67,86,15,33,85algorithm,huanglei,76,95,86,74,68,74,48algorithm,huangjiaju,85,75,86,85,85,74,86computer,huangdatou,48,58,67,86,15,33,85english,zhouqi,85,86,41,75,93,42,85,75,55,47,22english,huangbo,85,42,96,38,55,47,22algorithm,liutao,85,75,85,99,66computer,huangzitao,85,86,41,75,93,42,85math,wangbaoqiang,85,86,41,75,93,42,85computer,liujialing,85,41,75,21,85,96,14,74,86computer,liuyifei,75,85,62,48,54,96,15computer,liutao,85,75,85,99,66,88,75,91computer,huanglei,76,95,86,74,68,74,48english,liujialing,75,85,62,48,54,96,15math,huanglei,76,95,86,74,68,74,48math,huangjiaju,85,75,86,85,85,74,86math,liutao,48,58,67,86,15,33,85english,huanglei,85,75,85,99,66,88,75,91math,xuzheng,54,52,86,91,42,85,75math,huangxiaoming,85,75,85,99,66,88,75,91math,liujialing,85,86,41,75,93,42,85,75english,huangxiaoming,85,86,41,75,93,42,85algorithm,huangdatou,48,58,67,86,15,33,85algorithm,huangzitao,85,86,41,75,93,42,85,75`]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>MapReduce算法</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[求倒排索引1（修改默认输入组件以记录行号）]]></title>
    <url>%2F2018%2F01%2F18%2F%E6%B1%82%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%951%EF%BC%88%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E8%BE%93%E5%85%A5%E7%BB%84%E4%BB%B6%E4%BB%A5%E8%AE%B0%E5%BD%95%E8%A1%8C%E5%8F%B7%EF%BC%89%2F</url>
    <content type="text"><![CDATA[概念： 倒排索引（Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法， 被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。 它是文档检索系统中最常用的数据结构 求出每个关键词在哪个文档中的第几行出现了几次 输入数据格式： 有两个文件data1.txt和data2.txt，文件中的内容就是普通的文本。每个单词就是一个关键词 data1.txt的内容： zhangsan love zhouba lisi love zhengshi lisi love wujiu wangwu love zhaoliu lisi zhouba zhangsan sunqi data2.txt的内容： hello zhangsan hello zhouba hello lisi 输出数据格式： huangixaoming data1.txt,2,2;data1.txt,4,1;data2.txt,3,1 意义： 关键词 lisi 在第一份文档 data1.txt 中的第 2 行出现了 2 次 关键词 lisi 在第一份文档 data1.txt 中的第 4 行出现了 1 次 关键词 lisi 在第二份文档 data2.txt 中的第 3 行出现了 1 次 求得的最终结果： zhengshi data1.txt,2,1 hello data2.txt,3,1;data2.txt,2,1;data2.txt,1,1 zhangsan data2.txt,1,1;data1.txt,4,1;data1.txt,1,1 lisi data1.txt,4,1;data1.txt,2,2;data2.txt,3,1 wangwu data1.txt,3,1 zhaoliu data1.txt,3,1 love data1.txt,3,1;data1.txt,1,1;data1.txt,2,2 sunqi data1.txt,4,1 zhouba data1.txt,1,1;data1.txt,4,1;data2.txt,2,1 wujiu data1.txt,2,1 实现思想 在setup中获取文件名 修改默认组件以记录行号。 在大文件读取时，只能采用改写默认输入组件的方式进行。因为一个大文件会被切分成多个split并行执行，所以在mapper中声明一个属性记录行号的做法，会导致每一个块的每一行都从头算起，不可行 在map中以单词名称作为key；文件名，行号，出现次数作为value输出。在reduce中进行合并。 MapReduce主体逻辑123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125package exam_answer.three;import java.io.IOException;import java.util.HashMap;import java.util.Map;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class InvertedIndexMR_1 &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(InvertedIndexMR_1.class); job.setMapperClass(InvertedIndexMR_1_Mapper.class); job.setReducerClass(InvertedIndexMR_1_Reducer.class); //设置自定义的默认组件 job.setInputFormatClass(MyTextInputFormat.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.addInputPath(job, new Path("E:\\bigdatatest\\input")); Path outputPath = new Path("E:\\bigdatatest\\output"); FileSystem fs = FileSystem.get(conf); if (fs.exists(outputPath)) &#123; fs.delete(outputPath, true); &#125; FileOutputFormat.setOutputPath(job, outputPath); boolean isDone = job.waitForCompletion(true); System.exit(isDone ? 0 : 1); &#125; public static class InvertedIndexMR_1_Mapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; private String fileName = null; private Text outkey = new Text(); private Text outvalue = new Text(); //用于每一行中的单词计数 private Map&lt;String, Integer&gt; wordsCountMap = new HashMap&lt;String, Integer&gt;(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; /** * 获取文件名 */ InputSplit inputSplit = context.getInputSplit(); FileSplit fileSplit = (FileSplit)inputSplit; fileName = fileSplit.getPath().getName(); &#125; /** * map方法输出的key-value分别是： * key ：行号 * value ：word1 word2 word3 ... */ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] split = value.toString().split(" "); //把每一个词存入wordsCountMap并计数 for(String word : split)&#123; if(wordsCountMap.containsKey(word))&#123; wordsCountMap.put(word, wordsCountMap.get(word) + 1); &#125;else&#123; wordsCountMap.put(word, 1); &#125; &#125; //遍历wordsCountMap并输出key-value for(String word : wordsCountMap.keySet())&#123; outkey.set(word); outvalue.set(fileName + "," + key.get() + "," + wordsCountMap.get(word)); //输出格式：word 文件名，行号，个数 context.write(outkey, outvalue); &#125; //读取一行后清空HashMap wordsCountMap.clear(); &#125; &#125; public static class InvertedIndexMR_1_Reducer extends Reducer&lt;Text, Text, Text, Text&gt; &#123; private Text outvalue = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder sb = new StringBuilder(); //合并 for(Text val : values)&#123; sb.append(val.toString()).append(";"); &#125; //去除最后一个分号 String newStr = sb.toString().substring(0, sb.toString().length() - 1); outvalue.set(newStr); //输出格式：hello data2.txt,3,1;data2.txt,2,1;data2.txt,1,1 context.write(key, outvalue); &#125; &#125;&#125; 自定义输入组件自定义输入组件可以直接复制TextInputForMat，和LineRecordReader两个类再其上进行改写。 MyTextInputFormat.class 只需要把createRecordReader()方法中的返回值改成自定义的MyLineRecordReader。读取组件的主体逻辑是在RecordReader中实现的。 123456789@Overridepublic RecordReader&lt;LongWritable, Text&gt; createRecordReader(InputSplit split, TaskAttemptContext context) &#123; String delimiter = context.getConfiguration().get("textinputformat.record.delimiter"); byte[] recordDelimiterBytes = null; if (null != delimiter) recordDelimiterBytes = delimiter.getBytes(Charsets.UTF_8); //修改为自定义的类 return new MyLineRecordReader(recordDelimiterBytes);&#125; MyLineRecordReader.class因为只把map()的输入的key值从行偏移量改成行号，所以主体逻辑不需要变。nextKeyValue()方法是实现读取文件中的每一行数据的实现，在其中把原来设置key值的方法key.set(pos)改为key.set(lineCount)，当然先要声明一个属性private long lineCount = 1; 最后还要在while循环中对lineCount++；1234567891011121314151617181920212223242526272829303132333435363738394041424344private long lineCount = 1; public boolean nextKeyValue() throws IOException &#123; if (key == null) &#123; key = new LongWritable(); &#125;// key.set(pos); //把map输入的key设为行号 key.set(lineCount); if (value == null) &#123; value = new Text(); &#125; int newSize = 0; // We always read one extra line, which lies outside the upper // split limit i.e. (end - 1) while (getFilePosition() &lt;= end || in.needAdditionalRecordAfterSplit()) &#123; if (pos == 0) &#123; newSize = skipUtfByteOrderMark(); //对行数进行累加 lineCount++; &#125; else &#123; newSize = in.readLine(value, maxLineLength, maxBytesToConsume(pos)); pos += newSize; //对行数进行累加 lineCount++; &#125; if ((newSize == 0) || (newSize &lt; maxLineLength)) &#123; break; &#125; // line too long. try again LOG.info("Skipped line of size " + newSize + " at pos " + (pos - newSize)); &#125; if (newSize == 0) &#123; key = null; value = null; return false; &#125; else &#123; return true; &#125; &#125;]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[求倒排索引2]]></title>
    <url>%2F2018%2F01%2F18%2F%E6%B1%82%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%952%2F</url>
    <content type="text"><![CDATA[输入数据格式： 有两个文件data1.txt和data2.txt，文件中的内容就是普通的文本。每个单词就是一个关键词 data1.txt的内容： zhangsan love zhouba lisi love zhengshi lisi love wujiu wangwu love zhaoliu lisi zhouba zhangsan sunqi data2.txt的内容： hello zhangsan hello zhouba hello lisi 输出数据格式： zhangsan 4 data1.txt,3;data2.txt,1 hello 3 data2.txt,3 意义： 关键词 zhangsan 一共出现了4次，在第一份文档 data1.txt 中出现了 3 次 关键词 zhangsan 在第二份文档 data2.txt 中出现了 1 次 关键词 hello 在第二份文档 data2.txt 中出现了 3 次 实现思想 需要使用两个Mapreduce实现 第一个Mapreduce以单词和文件名作为map输出的key，在reduce中进行合并得到结果文件格式为： zhangsan data1.txt 3 关键词 文件名 一共出现3次 第二个Mapreduce。以关键词分组，关键词和文件名倒序排序，在reduce中合并每一个分组的内容。得到最终输出的结果文件： zhangsan 4 data1.txt,3;data2.txt,1 具体步骤 实现自定义类WordFileIndex.class实现WritableComparable接口 自定义分组WordFileIndexGroupComparator.class继承 map中把读取到每一条数据封装进对象 reduce对每一组内容进行拼接 两个MapReduce可以使用JobControl对象进行串联，这里为了方便展示各自逻辑，分开实现。 MapReduce11234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798package exam_answer.three;import java.io.IOException;import java.util.HashMap;import java.util.Map;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class InvertedIndexMR_2_1 &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(InvertedIndexMR_2_1.class); job.setMapperClass(InvertedIndexMR_1_Mapper.class); job.setReducerClass(InvertedIndexMR_1_Reducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path("E:\\bigdata\\input")); Path outputPath = new Path("E:\\bigdata\\output"); FileSystem fs = FileSystem.get(conf); if (fs.exists(outputPath)) &#123; fs.delete(outputPath, true); &#125; FileOutputFormat.setOutputPath(job, outputPath); boolean isDone = job.waitForCompletion(true); System.exit(isDone ? 0 : 1); &#125; public static class InvertedIndexMR_1_Mapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; private String fileName = null; private Text outkey = new Text(); private IntWritable outvalue = new IntWritable(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; InputSplit inputSplit = context.getInputSplit(); FileSplit fileSplit = (FileSplit)inputSplit; fileName = fileSplit.getPath().getName(); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // mapper阶段的业务逻辑编写之地 String[] split = value.toString().split(" "); for(String word : split)&#123; outkey.set(word+"\t"+fileName); outvalue.set(1); //输出格式：zhangsan data1.txt 1 context.write(outkey, outvalue); &#125; &#125; &#125; public static class InvertedIndexMR_1_Reducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; private IntWritable outvalue = new IntWritable(); int sum = 0; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; //统计每一个关键词在一个文件中一共出现的次数 for(IntWritable val : values)&#123; sum++; &#125; outvalue.set(sum); //输出格式：zhangsan data1.txt 3 context.write(key, outvalue); sum = 0; &#125; &#125;&#125; Mapreduce2自定义对象123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package exam_answer.three;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;public class WordFileIndex implements WritableComparable&lt;WordFileIndex&gt; &#123; private String word; private String fileName; private int count; public String getWord() &#123; return word; &#125; public void setWord(String word) &#123; this.word = word; &#125; public String getFileName() &#123; return fileName; &#125; public void setFileName(String fileName) &#123; this.fileName = fileName; &#125; public int getCount() &#123; return count; &#125; public void setCount(int count) &#123; this.count = count; &#125; public WordFileIndex(String word, String fileName, int count) &#123; super(); this.word = word; this.fileName = fileName; this.count = count; &#125; public WordFileIndex() &#123; &#125; @Override public String toString() &#123; return word + "\t" + count + "\t" + fileName; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(word); out.writeUTF(fileName); out.writeInt(count); &#125; @Override public void readFields(DataInput in) throws IOException &#123; // TODO Auto-generated method stub this.word = in.readUTF(); this.fileName = in.readUTF(); this.count = in.readInt(); &#125; /** * 按照 word 分组 按照 word 和 count 进行排序 */ @Override public int compareTo(WordFileIndex o) &#123; int compareTo = o.getWord().compareTo(this.word); if (compareTo == 0) &#123; return o.getCount() - this.getCount(); &#125; else &#123; return compareTo; &#125; &#125;&#125; 自定义分组12345678910111213141516package exam_answer.three;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class WordFileIndexGroupComparator extends WritableComparator&#123; public WordFileIndexGroupComparator()&#123; super(WordFileIndex.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; WordFileIndex aa = (WordFileIndex)a; WordFileIndex bb = (WordFileIndex)b; return aa.getWord().compareTo(bb.getWord()); &#125;&#125; Mapreduce2 主体逻辑123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103package exam_answer.three;import java.io.IOException;import java.util.HashMap;import java.util.Map;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class InvertedIndexMR_2_2 &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(InvertedIndexMR_2_2.class); job.setMapperClass(InvertedIndexMR_2_Mapper.class); job.setReducerClass(InvertedIndexMR_2_Reducer.class); job.setGroupingComparatorClass(WordFileIndexGroupComparator.class); job.setMapOutputKeyClass(WordFileIndex.class); job.setMapOutputValueClass(NullWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.addInputPath(job, new Path("E:\\bigdata\\output")); Path outputPath = new Path("E:\\bigdata\\output_2"); FileSystem fs = FileSystem.get(conf); if (fs.exists(outputPath)) &#123; fs.delete(outputPath, true); &#125; FileOutputFormat.setOutputPath(job, outputPath); boolean isDone = job.waitForCompletion(true); System.exit(isDone ? 0 : 1); &#125; /* 输入value ： huangxiaoming mapreduce-4-1.txt 3 huangxiaoming mapreduce-4-2.txt 1 输出key-value： key : WordFileIndex value : null */ public static class InvertedIndexMR_2_Mapper extends Mapper&lt;LongWritable, Text, WordFileIndex, NullWritable&gt; &#123; private WordFileIndex outkey = new WordFileIndex(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] split = value.toString().split("\t"); outkey.setWord(split[0]); outkey.setFileName(split[1]); outkey.setCount(Integer.parseInt(split[2])); context.write(outkey, NullWritable.get()); &#125; &#125; public static class InvertedIndexMR_2_Reducer extends Reducer&lt;WordFileIndex, NullWritable, Text, NullWritable&gt; &#123; private Text outkey = new Text(); // 关键词出现的总次数 int sum = 0; @Override protected void reduce(WordFileIndex key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder wordIndexSB = new StringBuilder(); // reducer阶段的业务逻辑编写之地 for(NullWritable val : values)&#123; // 求出总次数 sum += key.getCount(); //拼接以关键词分组的每一组的值，关键词次数是降序排序的 wordIndexSB.append(key.getFileName()).append(",").append(key.getCount()).append(";"); &#125; //除去最后一个分号 String newWordIndexStr = wordIndexSB.toString().substring(0, wordIndexSB.toString().length() - 1); outkey.set(key.getWord()+"\t" + sum + "\t" + newWordIndexStr); /* * 最终输出格式 * zhangsan 4 data1.txt,3;data2.txt,1 * hello 3 data2.txt,3 */ context.write(outkey, NullWritable.get()); sum = 0; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计用户在同一地点停留时长]]></title>
    <url>%2F2018%2F01%2F18%2F%E7%BB%9F%E8%AE%A1%E7%94%A8%E6%88%B7%E5%9C%A8%E5%90%8C%E4%B8%80%E5%9C%B0%E7%82%B9%E5%81%9C%E7%95%99%E6%97%B6%E9%95%BF%2F</url>
    <content type="text"><![CDATA[描述： 对同一个用户，在同一个位置，连续的多条记录进行合并 合并原则：开始时间取最早的，停留时长加和 字段： userID, locationID, time, duration 数据样例： user_a location_a 2018-01-01 08:00:00 60 user_a location_a 2018-01-01 09:00:00 60 user_a location_a 2018-01-01 11:00:00 60 user_a location_a 2018-01-01 12:00:00 60 输出结果： user_a location_a 2018-01-01 08:00:00 120 user_a location_a 2018-01-01 11:00:00 120 测试数据： user_a location_a 2018-01-01 08:00:00 60 user_a location_a 2018-01-01 09:00:00 60 user_a location_a 2018-01-01 11:00:00 60 user_a location_a 2018-01-01 12:00:00 60 user_a location_b 2018-01-01 10:00:00 60 user_a location_c 2018-01-01 08:00:00 60 user_a location_c 2018-01-01 09:00:00 60 user_a location_c 2018-01-01 10:00:00 60 user_b location_a 2018-01-01 15:00:00 60 user_b location_a 2018-01-01 16:00:00 60 user_b location_a 2018-01-01 18:00:00 60 结果数据： user_a location_a 2018-01-01 08:00:00 120 user_a location_a 2018-01-01 11:00:00 120 user_a location_b 2018-01-01 10:00:00 60 user_a location_c 2018-01-01 08:00:00 180 user_b location_a 2018-01-01 15:00:00 120 user_b location_a 2018-01-01 18:00:00 60 实现思想 以用户id和位置id为分组，以开始时间升序排序 在reduce中进行判断，在每一组中判断，前后两条数据是否是连续的（前一条数据的开始时间+持续时间，是否等于后一条的开始时间），如果连续则合并持续时间，最后输出最早的开始时间和持续时间和 实现代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134package exam_answer.two;import java.io.IOException;import java.text.ParseException;import java.text.SimpleDateFormat;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class UserLocationMR &#123; public static void main(String[] args) throws Exception &#123; // 指定hdfs相关的参数 Configuration conf = new Configuration(); // conf.set("fs.defaultFS", "hdfs://hadoop02:9000"); // System.setProperty("HADOOP_USER_NAME", "hadoop"); Job job = Job.getInstance(conf); // 设置jar包所在路径 job.setJarByClass(UserLocationMR.class); // 指定mapper类和reducer类 job.setMapperClass(UserLocationMRMapper.class); job.setReducerClass(UserLocationMRReducer.class); // 指定maptask的输出类型 job.setMapOutputKeyClass(UserLocation.class); job.setMapOutputValueClass(NullWritable.class); // 指定reducetask的输出类型 job.setOutputKeyClass(UserLocation.class); job.setOutputValueClass(NullWritable.class); job.setGroupingComparatorClass(UserLocationGC.class); // 指定该mapreduce程序数据的输入和输出路径 Path inputPath = new Path("D:\\bigdata\\userlocation\\input"); Path outputPath = new Path("D:\\bigdata\\userlocation\\ouput"); FileSystem fs = FileSystem.get(conf); if (fs.exists(outputPath)) &#123; fs.delete(outputPath, true); &#125; FileInputFormat.setInputPaths(job, inputPath); FileOutputFormat.setOutputPath(job, outputPath); // 最后提交任务 boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion ? 0 : 1); &#125; private static class UserLocationMRMapper extends Mapper&lt;LongWritable, Text, UserLocation, NullWritable&gt; &#123; UserLocation outKey = new UserLocation(); /** * value = user_a,location_a,2018-01-01 12:00:00,60 */ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] split = value.toString().split(","); outKey.set(split); context.write(outKey, NullWritable.get()); &#125; &#125; private static class UserLocationMRReducer extends Reducer&lt;UserLocation, NullWritable, UserLocation, NullWritable&gt; &#123; SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); UserLocation outKey = new UserLocation(); /** * user_a location_a 2018-01-01 08:00:00 60 * user_a location_a 2018-01-01 09:00:00 60 * * user_a location_a 2018-01-01 11:00:00 60 * user_a location_a 2018-01-01 12:00:00 60 */ @Override protected void reduce(UserLocation key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; for (NullWritable nvl : values) &#123; count++; // 如果是这一组key-value中的第一个元素时，直接赋值给outKey对象。基础对象 if (count == 1) &#123; // 复制值 outKey.set(key); &#125; else &#123; // 有可能连续，有可能不连续， 连续则继续变量， 否则输出 long current_timestamp = 0; long last_timestamp = 0; try &#123; // 这是新遍历出来的记录的时间戳 current_timestamp = sdf.parse(key.getTime()).getTime(); // 这是上一条记录的时间戳 和 停留时间之和 last_timestamp = sdf.parse(outKey.getTime()).getTime() + outKey.getDuration() * 60 * 1000; &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; // 如果相等，证明是连续记录，所以合并 if (current_timestamp == last_timestamp) &#123; outKey.setDuration(outKey.getDuration() + key.getDuration()); &#125; else &#123;//不相等，不能合并，直接输出。 // 先输出上一条记录 context.write(outKey, nvl); // 然后再次记录当前遍历到的这一条记录 outKey.set(key); &#125; &#125; &#125; // 最后无论如何，还得输出最后一次 context.write(outKey, NullWritable.get()); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce自定义输入输出组件]]></title>
    <url>%2F2017%2F12%2F18%2FMapReduce%E8%87%AA%E5%AE%9A%E4%B9%89%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%BB%84%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[多个小文件合并，一个key-value，value是小文件的所有内容。 套路：模仿org.apache.hadoop.mapreduce.lib.input.LineRecordReader和org.apache.hadoop.mapreduce.lib.input.TextInputFormat 把输入控件设置成自定义的控件类 1job.setInputFormatClass(MyAllFileInputFormat.class); 实现自定义控件继承FileInputFormat&lt;Text, LongWritable&gt;这里的泛型就是读取文件时的key和value的类型。 实现createRecordReader方法，该方法返回一个RecordReader对象，该对象就是真正读取文件内容的对象，所以，在该方法中实例化一个自定义的RecordReader对象，并把split和context参数传给他。 123456789101112131415161718public class MyAllFileInputFormat extends FileInputFormat&lt;Text, LongWritable&gt;&#123; @Override public RecordReader&lt;Text, LongWritable&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; //实例化一个 MyAllFileRecodReader reader = new MyAllFileRecodReader(); //split参数和context都是框架自动传入的，把这两个参数传给reader进行处理，以便获取相关信息 reader.initialize(split, context); return reader; &#125; /** * 给定的文件名可拆分吗?返回false确保单个输入文件不会被分割。以便Mapper处理整个文件。 */ @Override protected boolean isSplitable(JobContext context, Path filename) &#123; return false; &#125;&#125; 实现自定义RecordReader。 重点： nextKeyValue()方法中实现读取文件逻辑。 在initialize()可以接受到FileInputFormat组件中传来的InputSplit和Context对象，可以从其中获取到文件信息，和配置文件等一些列信息。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public class MyAllFileRecodReader extends RecordReader&lt;Text, LongWritable&gt;&#123; //用于存储文件系统输入流 private FSDataInputStream open = null; //保存文件长度 private int fileSplitLength = 0; /** * 当前的MyAllFileRecodReader读取到的一个key-value */ private Text key = new Text(); private LongWritable value = new LongWritable(); @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; //通过InputSplit对象获取文件路径 FileSplit fileSplit = (FileSplit)split; Path path = fileSplit.getPath(); //获取文件长度 fileSplitLength = (int)fileSplit.getLength(); //通过context对象获取到配置文件信息，通过配置文件获取到一个当前文件系统 Configuration configuration = context.getConfiguration(); FileSystem fs = FileSystem.get(configuration); //获取文件系统的一个输入流 open = fs.open(path); &#125; /** * 已读标记 * 如果为false，表示还没有进行读取 * 在需求中一个mapTask只处理一个小文件，一个mapTask最终只需要读取一次就完毕 * 如果一个文件读取完毕了，那么就把isRead这个变量标记为true */ private boolean isRead = false; /** * 实现读取规则：逐文件读取 */ @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; //如果没有读取过文件就进入 if(!isRead)&#123; //准备一个字节数组长度为文件的长度 byte[] buffer = new byte[fileSplitLength]; //一次性把真个文件读入字节数组中 IOUtils.readFully(open, buffer); //把读取到的文件传给key key.set(buffer, 0, fileSplitLength); //设置已读标记为true isRead = true; //返回读取一个文件成功标记 return true; &#125;else&#123; return false; &#125; &#125; //获取key的方法 @Override public Text getCurrentKey() throws IOException, InterruptedException &#123; return key; &#125; //获取当前value值 @Override public LongWritable getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; /** * 获取数据的处理进度的 */ @Override public float getProgress() throws IOException, InterruptedException &#123; //已读为真返回1.0，没有读返回0 return isRead ? 1.0F : 0F; &#125; @Override public void close() throws IOException &#123; //安静地关闭输入流 IOUtils.closeQuietly(open); &#125;&#125; 最后记得修改mapper中的输入key和value的类型 123456public static class InputMapper extends Mapper&lt;Text, LongWritable, Text, NullWritable&gt;&#123; @Override protected void map(Text key, LongWritable value, Context context) throws IOException, InterruptedException &#123; context.write(key, NullWritable.get()); &#125; &#125; 自定义输出组件给定数据条目为：math,liutao,86（课程名，姓名，考试分数）。 需求：分数超过60的同学输出到文件夹jige中，低于60分的输出到文件夹bujige中。 实现方法：改写数据输出组件的输出规则。 套路：模仿org.apache.hadoop.mapreduce.lib.output.TextOutputFormat&lt;K, V&gt;和org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.LineRecordWriter&lt;K, V&gt; 设置输出组件类为自定义类job.setOutputFormatClass(MyMultipePathOutputFormat.class); 实现自定义输出组件，继承FileOutputFormat，实现getRecordWriter()方法，此方法需要返回一个RecordWriter，这就是真正处理输出流的类，输出流的核心逻辑就在其中。 123456789101112131415public class MyMultipePathOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;&#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException &#123; //获得当前的文件系统传给自定义的RecordWriter组件 Configuration configuration = job.getConfiguration(); FileSystem fs = FileSystem.get(configuration); try &#123; //返回一个RecordWriter正在处理输出数据的组件 return new MyMutiplePathRecordWriter(fs); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 实现自定义组件MyRecordWriter继承RecordWriter实现核心方法write() 123456789101112131415161718192021222324252627282930313233343536373839404142public class MyMutiplePathRecordWriter extends RecordWriter&lt;Text, NullWritable&gt;&#123; //声明要输出的两个路径 private DataOutputStream out_jige; private DataOutputStream out_bujige; public MyMutiplePathRecordWriter(FileSystem fs) throws Exception &#123; //创建系统输出流 out_jige = fs.create(new Path("E:\\bigdata\\cs\\jige\\my_output_jige.txt")); out_bujige = fs.create(new Path("E:\\bigdata\\cs\\bujige\\my_output_bujige.txt")); &#125; /** * 实现写出方法，根据需要写出的格式自定义 */ @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; //接受到的key格式为：course + "\t" + name + "\t" + avgScore String keyStr = key.toString(); String[] split = keyStr.split("\t"); //获取到平均分字段 double score = Double.parseDouble(split[2]); //没一行数据加入个换行符 byte[] bytes = (keyStr + "\n").getBytes(); //如果平均分大于60就用DataOutputStream写出到jige目录 if(score &gt;= 60)&#123; out_jige.write(bytes, 0, bytes.length); &#125;else&#123;//小于60分的写道bujige目录 out_bujige.write(bytes, 0, bytes.length); &#125; &#125; /** * 在close方法中关闭输出流。 */ @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; IOUtils.closeQuietly(out_jige); IOUtils.closeQuietly(out_bujige); &#125;&#125;]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[两张表的连接-reduce join]]></title>
    <url>%2F2017%2F11%2F18%2F%E4%B8%A4%E5%BC%A0%E8%A1%A8%E7%9A%84%E8%BF%9E%E6%8E%A5-reduce-join%2F</url>
    <content type="text"><![CDATA[要点： 两个表中有相同字段，把相同字段的条目，发送到reduce进行拼接 用文件名来区分map中取到的条目是来自哪张表，以便于切分 一个文件块一个split只需要在setup()方法中获取一次文件名就可以 在reduce中进行两张表的拼接，会出现数据倾斜，不适合做大文件操作。 mapper12345678910111213141516171819202122232425262728293031323334353637/**users.dat 数据格式为： 2::M::56::16::70072对应字段中文解释：用户id，性别，年龄，职业，邮政编码ratings.dat 数据格式为： 1::1193::5::978300760对应字段中文解释：用户ID，电影ID，评分，评分时间戳 */public static class MR_Mapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; String fileName; /** * 在map初始化方法中获取文件名用于区分表 */ @Override protected void setup(Context context) throws IOException, InterruptedException &#123; InputSplit inputSplit = context.getInputSplit(); FileSplit fileSplit = (FileSplit)inputSplit; fileName = fileSplit.getPath().getName(); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] split = value.toString().split("::"); String str=""; String id = ""; if(fileName.equals("users.dat"))&#123;//文件名为users.dat为第一张表 id = split[0]; str = 1+"\t" + split[1]+"\t"+split[2]+"\t"+split[3]+"\t"+split[4]; &#125;else&#123; id = split[0]; str = 0+"\t" + split[1]+"\t"+split[2]+"\t"+split[3]; &#125; //以两张表中相同的字段作为outkey输出给reduce outKey.set(id); outValue.set(str); context.write(outKey, outValue); &#125;&#125; reduce阶段：12345678910111213141516171819202122232425262728293031323334353637383940public static class MR_Reducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123; //存储输出value Text outValue = new Text(); //用于存储user表中的字段。 ArrayList&lt;String&gt; userList = new ArrayList&lt;&gt;(); //用于存储user表中的字段。 ArrayList&lt;String&gt; rateList = new ArrayList&lt;&gt;(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; //分别取出两张表中的条目 for(Text v:values)&#123; String[] split = v.toString().split("\t"); if(split[0].equals("1"))&#123;//标识1，为users.dat表 String temp = split[1]+"\t"+split[2]+"\t"+split[3]+"\t"+split[4]; userList.add(temp); &#125;else&#123; String temp = split[1]+"\t"+split[2]+"\t"+split[3]; rateList.add(temp); &#125; &#125; //对两张表进行连接（内连接操作） for(String user : userList)&#123; for(String rate:rateList)&#123; outValue.set(user + "\t--"+rate); context.write(key, outValue); &#125; &#125; //左外链接操作（右做相反操作就可以了） if(userList.size()&gt;0&amp;&amp;rateList.size()==0)&#123; for(String user : userList)&#123; outValue.set(user); context.write(key, outValue); &#125; &#125; userList.clear(); rateList.clear(); &#125;&#125;]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>MapReduce算法</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小表连接大表-map join]]></title>
    <url>%2F2017%2F11%2F18%2F%E5%B0%8F%E8%A1%A8%E8%BF%9E%E6%8E%A5%E5%A4%A7%E8%A1%A8-map-join%2F</url>
    <content type="text"><![CDATA[要点： 只能做大表连接小表的操作， 把小表加载当前执行mapTask任务的节点上 只适合实现大表对小表内连接和左外连接，不能实现有外连接。 只能在集群中进行测试。如报java.lang.InterruptedException和java.lang.CaughtException可忽略 分布式组件分布小文件到节点本地的路径： data/hadoopdata/nm-local-dir/filecache/10/ 添加小文件123//设置要分布式的缓存到mapTask节点文件路径//利用DistributedCache分布式缓存组件，把文件分发进行map操作的节点本地job.addCacheFile(new URI(args[2])); mapper实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/**users.dat 数据格式为： 2::M::56::16::70072对应字段中文解释：用户id，性别，年龄，职业，邮政编码 */public static class MR_Mapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; //用于存储小表数据 HashMap&lt;String , String&gt; users = new HashMap&lt;&gt;(); @Override protected void setup(Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; //获得内存本地缓存的小文件的路径 Path[] localCacheFiles = context.getLocalCacheFiles(); String usersFilePath = localCacheFiles[0].toUri().toString(); //通过字符输入流读取进users中 BufferedReader br = new BufferedReader(new FileReader(new File(usersFilePath))); String line=null; while((line = br.readLine())!=null)&#123; String[] split = line.split("::"); String id = split[0]; String mapValue = split[1]+"\t"+split[2]+"\t"+split[3]+"\t"+split[4]; //存储到内存 users.put(id, mapValue); &#125; br.close(); &#125; //ratings.dat 数据格式为： 1::1193::5::978300760 //对应字段中文解释：用户ID，电影ID，评分，评分时间戳 Text outValue = new Text(); Text outKey = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] split = value.toString().split("::"); String userId = split[0]; outKey.set(userId); String strValue = split[1]+"\t"+split[2]+"\t"+split[3]; //如果HashMap中存在当前userId就进行数据拼接 if(users.containsKey(userId))&#123; String mapValue = users.get(userId); outValue.set(strValue+"\t"+mapValue); context.write(outKey, outValue); &#125;else&#123;//做左外连接（不能做右外连接） outValue.set(strValue + "\t"+""+"0"+"\t"+"0"+"\t"+"00000"); &#125; &#125; &#125;]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>MapReduce算法</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[插入排序，二分查找，二维数组打印杨辉三角]]></title>
    <url>%2F2017%2F10%2F18%2F%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F%EF%BC%8C%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%EF%BC%8C%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E6%89%93%E5%8D%B0%E6%9D%A8%E8%BE%89%E4%B8%89%E8%A7%92%2F</url>
    <content type="text"><![CDATA[插入排序。123456789101112131415161718192021222324//方式一：int arr = &#123;3,11,6,22,1,44&#125;;for(int i=1; i&lt;arr.length; i++)&#123; int temp = arr[i]; int j = i; while(j&gt;0&amp;&amp;arr[j-1]&gt;temp)&#123;//前面的数大于i处的值就往后覆盖，一直到temp的值比这个值小的时候停止 arr[j] = arr[j-1]; j--; &#125; arr[j] = temp;//把temp的值赋给前面元素比它大的位置&#125;//方式二for(int i = 1; i&lt;arr.length; i++)&#123; for(int j=0;j&lt;i;j++)&#123; if(arr[j]&gt;arr[i])&#123; int temp = arr[i]; for(int k=i;k&gt;j;k--)&#123; arr[k] = arr[k-1]; &#125; arr[j]=temp; &#125; &#125;&#125; 有序数组的二分查找。 123456789101112131415161718192021222324public class BinarySearch &#123; public static void main(String[] args) &#123; int[] arr = &#123;1, 5, 7, 12, 19, 22, 25, 29, 33&#125;; int start = 0; int end = arr.length-1; int middle; int value = 25; while (start&lt;end) &#123; middle = (start + end) / 2; if (arr[middle] == value) &#123; System.out.println(middle); break; &#125; if (arr[middle] &gt; value) &#123; end = middle-1; &#125; if (arr[middle] &lt; value) &#123; start = middle+1; &#125; &#125; &#125;&#125; 利用不规则的二维数组存储杨辉三角 1234567891011121314151617int[][] arr = new int[8][]; for(int i=0; i&lt;arr.length; i++)&#123; arr[i] = new int[i+1]; &#125; for(int i=0; i&lt;arr.length; i++)&#123; for(int j=0; j&lt;arr[i].length; j++)&#123; if(j==0||j==i)&#123; arr[i][j] = 1; System.out.print(arr[i][j]); &#125;else&#123; arr[i][j] = arr[i-1][j-1]+arr[i-1][j]; System.out.print(arr[i][j]); &#125; &#125; System.out.println(); &#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[两种自由]]></title>
    <url>%2F2017%2F09%2F26%2F%E4%B8%A4%E7%A7%8D%E8%87%AA%E7%94%B1%2F</url>
    <content type="text"><![CDATA[精神自由liberal arts一词来源于西方属于西学，有的翻译叫“通识教育”、“素质教育”、“人文教育”、“博雅教育”等，但更确切的讲应该译作“自由技艺”。在古代她是统治者的学问，而在现代是一种培养具有批判性思维，独立人格的学问，也可以叫做拒绝被统治的学问。 这些学问和那些可以直接从事生产赚钱的技术不同，她们是一种软实力，这些软实力中最重要的有三个：批判性思维、交流、解决问题的能力。 批判性思维简单的讲就是对一件事有自己的主见。遇到一件事不随大流，不跟风，要经过调查分析，逻辑判断，多视角思考之后才得出自己的判断。 交流这里讲的是广义上的交流，包括表达自己思想，说服别人，演讲写作等等。在交流中要有自己的风格，也就是人格魅力，他是通过自由技艺的训练而获得的，它不单是个性，还要和别人产生共鸣。 解决问题的能力说的是当你现在遇到问题的时候你要能从你学习到众多知识中找到性质相似的案例，从而知道怎么很好的解决当下的问题。 有了批判性思维你能建立正确的认知。学会有风格的交流你能用你的认知影响别人，再加上一篮子解决问题的方法，那你将不再是一个工具而是一个独立的人了。 这些技艺并不是客观世界的科学，而是主观的体验和看法。他来源于各种人文、艺术学科。 所以，如果你只看专业书籍的话你最终还是工具，乞丐中的王者还是乞丐。只有加上人文和艺术你才能成为自由人。 财富自由怎么获得财富自由呢？一句话就是通过刻意练习，让你的一门手艺达到前5%。或者你把两项达到前25%的手艺组合起来使用。 那下面重点就是怎么样进行刻意练习了。 针对性重复练习。 套路，也就是拿来主义。 要站在前人的肩膀上，不要重复发明轮子。 最重要的套路有两种： 行业经验，书本上找不到，而是行业前辈总结出的经验。你需要找老司机学套路。 学科的概念，一个学科是靠几个大的概念堆出来的，要围绕着这些大的概念把他们弄清楚。比如，经济学20世纪最重要的概念就是发现了交易成本，物理学是发现了熵。只要你了解这些概念清晰的内涵和外延，你就能把这个学科整块的知识拿到手。 拆解练习。不能以赛代练，把大的知识体系拆成为一个个小模块，把复杂问题拆分成简单的小问题逐个攻破。这才有出路。 不断重复重复。 持续做你不会做的事人通常生活在三个区域的划分：舒适区、学习区、恐慌区。大多数人都是生活在舒适区的，但只有学习区才能使人进步，然而想要脱离舒适区是痛苦的。 主动脱离舒适区进入学习情。 刀削面师傅花哨的削面动作在现代只能算是熟练的简单技能，要掌握综合性的复杂技能，就不能一味停留在舒适区不能自拔。不要让自己进入下意识行动状态，要永远挑战意外，不要让自己失控，要时刻用理智指挥自己，强制进入学习区。 被动脱离舒适区进入学习区 好的学习环境，是能获得即使反馈的环境。每一个微小的进步，外界环境都给你一个反馈告诉你是对还是错？建立即时反馈系统，可以被动的让你进入学习区。 总之就是不断重复做你不会做的事，这是很挑战人性本能的活动，但是只要方法得当，你坚持上一段比较长的时间，人人都能做到。 按照马斯洛的需求理论，你必须先追求财富自由，才可能追求精神自由。但这不是绝对的，二者是相辅相成，在追求财富自由的间隙学习一点自由技艺，只会加快你实现财富自由的速度。另外，财富自由可以与生俱来，但精神自由是每个人都需要独立完成的。 我将会用一生的时间去追求这两项自由。]]></content>
      <categories>
        <category>成为你自己</category>
      </categories>
      <tags>
        <tag>人生</tag>
        <tag>自由</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[怎样做到100分]]></title>
    <url>%2F2017%2F09%2F26%2F%E6%80%8E%E6%A0%B7%E5%81%9A%E5%88%B0100%E5%88%86%2F</url>
    <content type="text"><![CDATA[现在很流行一万小时定律，就是你要在某个领域成为专家那只需要在这个领域经过一万小时的练习就可以了。那真的是这样的吗？真的需要一万小时吗？一万小时就会成为专家吗？其实这都不一定，就像一个出租车司机，已经积累了1万小时的里程但是他还是成为不了赛车手一样。罗辑思维的是这期节目告诉你到底应该怎么做才能练成一个高手。建议在原文链接里看完这期节目后再回来看这篇文章会收获更大。 刻意练习一、练习——针对性重复练习。 套路，拿来主义。 前人的知识封装成一个知识罐头，抽象为简洁的东西，拿来就用。最重要的套路： 行业经验，书本上找不到，而是行业前辈总结出的经验。找老司机学套路。 学科的概念，一个学科是靠几个大的概念堆出来的，要围绕着这些大的概念把他们弄清楚。经济学20世纪最重要的概念就是发现了交易成本，物理学是发现了熵。只要你了解这些概念清晰的内涵和外延，你就能把这个学科整坨的知识拿到手。 拆解练习。不能以赛代练，要要把大的知识体系拆碎成为一个个小模块，一个个小知识罐头，分别练习。 最重要的事是——重复。 二、刻意——持续做你不会做的事 三个区域的划分：舒适区—学习区—恐慌区 学习区：脱离舒适区。 农耕时代的勤奋苦练就能成为高手的技能如刀削面师傅等待在现代社会是简单技能，但是现在社会是分工合作的复杂体，要掌握的技能是综合性的复杂技能，不能一味停留在舒适区不能自拔。真正会读书的人是看对自己有挑战的书的人，这种人才是真正有效的。高手是不会让自己进入下意识状态。永远挑战意外永远不会允许自己失控。 被动的脱离舒适区，进入学习区 好的学习环境，是能获得即使反馈的环境。每一个微小的进步，外界环境都给你一个反馈告诉你对错，是否要继续练习。要建立即时反馈系统。 总结：学习的本质就是脱离舒服，没有轻松省力的捷径，学习注定是痛苦的。 三、学习的真相 互联网的信息结构是超文本连接而不是学习。他要人不断的做决策，做选择，而这些决策是被别人引诱而做出来的。（谷歌的核心战略的让用户快进快出），选择是由前额叶做出的，选择并不是在学习。 真正的学习是，不断把新的信息和自身原有的信息结构做缝接，高手的特点就是任何获得的新信息马上能和自己大脑中长期存储的记忆建立联系，信息群被迅速同时激活。做笔记是把学到的东西和自己的知识结构做缝接的过程。收藏的文字要写心得，这是为了与新知识形成互动，把新知识缝接到我原来的知识结构上。知识，是信息之间形成结构进入我们的大脑库存，这个库存不是装东西的存，而是一种体系结构，知识是一种生长出来带结构的东西。 怎样使用工具。有了互联网，人类让自己堕落的更浅薄。我们不能躺在工具产生的便利性上睡大觉挥霍时间在舒适区享受，而是要踏着新工具新技术进一步探索世界，开拓新边疆，向学习区进发。]]></content>
      <categories>
        <category>社会科学</category>
      </categories>
      <tags>
        <tag>人生</tag>
        <tag>刻意练习</tag>
        <tag>学习</tag>
      </tags>
  </entry>
</search>
